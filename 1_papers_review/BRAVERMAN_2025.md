# Towards a Probabilistic Framework for Analyzing and Improving LLM-Enabled Software

## Abstract

- Los sistemas basados en LLMs son potentes pero **no deterministas.** Es dif√≠cil confiar plenamente en lo que generan o verificar formalmente su comportamiento.
- La **idea central** del paper es: no mirar los *outputs* del LLM como cadenas de texto, sino como ***clusters of semantically equivalent outputs***. 
	- Por ejemplo: todas las respuestas que formalizan una misma propiedad en un c√≥digo.
- ***"Transference Models (TMs):*** concepto definido por los autores, son **LLM center modules** que reciben una entrada (ej. una descripci√≥n) y generan una salida (ej. especificaci√≥n formal). Analizar como se comportan los TMs bajo el LLM es la base del trabajo.
- La **aplicaci√≥n concreta** elegida en el paper es, dado un *docstring*, el modelo debe generar **precondiciones** y **postcondiciones** formales en Dafny.
- Al mirar no solo qu√© responde el modelo, sino c√≥mo reparte la probabilidad entre distintas interpretaciones, se pueden **identificar patrones de error sistem√°ticos**, por ejemplo, cuando el modelo siempre genera una postcondici√≥n d√©bil. 

> Los sistemas que usan LLMs son d√≠ficiles de verificar y depurar. Proponen un enfoque no analiza solo las respuesta, sino la distribuci√≥n de probabilidad **sobre clases sem√°nticas**. Esto los ayuda a identificar errores sistem√°ticos, como respuestas incorrectas con alta confianza. Lo ilustran con el problema de transformar lenguaje natural en especificaciones formales (*autoformalizaci√≥n*). 


## I. Introducci√≥n

Los LLMs (como GPT-4 y Gemini) funcionan muy bien en muchas tareas gracias a que pueden seguir instrucciones (*"instruction following"*) y aprender del contexto. Esto incluye estrategias como:
- **Zero-shot:** Darle una instrucci√≥n y esperar que entiende qu√© hacer sin ejemplos.
- **Few-shot:** Darle algunos ejemplos en el prompt.

Esta flexibilidad hace que los LLMs se usen cada vez m√°s en aplicaciones reales (*downstream tasks* = tareas derivadas), simplemente d√°ndoles prompts. La **alucinaci√≥n**, es uno de los principales problemas que esto trae. Esto es, el modelo genera cosas que suenan bien, pero son falsas o inv√°lidas. 

Se necesita **entender el comportamiento del sistema.** En el √°mbito de ingenier√≠a de software (de estas cosas), el comportamiento estoc√°stico del modelo es visto m√°s como un problema (por ejemplo, tests que fallan a veces y otras no: *flaky tests*), y no como algo que se pueda analizar o aprovechar

---
‚ö†Ô∏è Se est√° ignorando que el LLM no da una sola respuesta, sino una **distribuci√≥n sobre muchas posibles**??

--- 

La propuesta central es:
1. Identificar qu√© partes del sistema est√°n usando LLMs para transformar informaci√≥n (los **Transference Models**, TMs).
2. Luego, hay que modelar no las salidas en s√≠, sino la **distribuci√≥n de probabilidad sobre clases de significados equivalentes** (lo que llaman *meaning classes*).


**TESIS:** Si entend√©s c√≥mo el LLM reparte probabilidad entre interpretaciones posibles, podemos analizar y mejorar el sistema.


---

**HIP√ìTESIS:**
Si el modelo da una respuesta equivocada **con alta confianza** (concentraci√≥n en una clase err√≥nea), probablemente esa clase de error **tiene una explicaci√≥n sem√°ntica clara**, y por lo tanto se puede **diagnosticar y corregir** el TM.

> Cuando el modelo se equivoca con confianza, dice mucho sobre sus puntos d√©biles y se puede usar eso para mejorar(lo).


### Mis preguntitas:

- El agrupamiento de clases, no depende much√≠simo de la tarea??
- Si una tarea no tiene una √∫nica "clase correcta", c√≥mo se define la alineaci√≥n??
- Es viable para tareas con millones de posibles salidas?

## II. Preliminares

### A. Large Language Models

#### ¬øQu√© es un LLM desde el punto de vista t√©cnico?

Desde el punto de vista t√©cnico, un LLM, b√°sicamente es una **funci√≥n probabil√≠stica param√©trica**. Es decir:
- Toma un input $x$ (un texto anterior, tambi√©n llamado "contexto").
- Y devuelve una distribuci√≥n de probabilidad sobre posibles pr√≥ximos tokens $t_k$:
$$ P(t_k | x)$$ Esto significa que el modelo no genera "una respuesta", sino una **distribuci√≥n sobre todas las respuestas posibles,** token por token.

---

#### ¬øC√≥mo se entrena un LLM?

A trav√©s de _next-token prediction_ (predicci√≥n del pr√≥ximo token). Se entrena con **corpus masivos** de texto, ajustando sus par√°metros para minimizar un error (usualmente entrop√≠a cruzada) entre:

- lo que el modelo predice como pr√≥ximo token, y
- lo que realmente sigue en el texto de entrenamiento.

‚ö†Ô∏è Esto refuerza la idea de que el modelo aprende **probabilidades de continuaci√≥n**, no reglas expl√≠citas o verdades l√≥gicas.

---
#### ¬øC√≥mo se usa un LLM en la pr√°ctica?

Una vez entrenado, el modelo puede usarse para generar texto. Para hacerlo, necesita un **decoding strategy**, como:

- _Greedy_: elige siempre el token m√°s probable.
- _Sampling_: elige aleatoriamente seg√∫n la distribuci√≥n.
- _Top-k_ / _Top-p_: limita la elecci√≥n a los tokens m√°s probables.
- _Temperature_: controla cu√°n "creativo" es el modelo (m√°s temperatura = m√°s aleatoriedad).

Esto es crucial porque **el mismo input puede generar diferentes salidas** si el decoding es estoc√°stico.

---
#### Conexi√≥n con el paper

Los autores enfatizan que si bien muchas aplicaciones usan el LLM como una caja negra, ellos quieren **modelar esta distribuci√≥n** para entender el comportamiento del sistema. No les interesa solo ‚Äúqu√© salida dio‚Äù, sino:

> **¬øCon qu√© probabilidad eligi√≥ esa salida sobre otras posibles equivalentes?**

Esto los lleva a analizar _distribuciones sobre clases sem√°nticas_, no solo sobre tokens.


### B .Autoformalization

#### ¬øQu√© es la autoformalizaci√≥n?

Autoformalizaci√≥n es el proceso de **convertir texto natural en especificaciones formales.** Es decir, traducir algo como:

> `"""Devuelve el √≠ndice del primer elemento igual a x en el array."""`

en algo como (en lenguaje formal como **Dafny**)

``` dafny
requires a != null
ensures forall i :: 0 <= i < result ==> a[i] != x
ensures a[result] == x
```

Esta tarea requiere **comprensi√≥n sem√°ntica**, **capacidad de inferencia l√≥gica** y conocimiento del dominio del c√≥digo. No es simplemente traducir palabra por palabra.


---
#### ¬øPor qu√© es importante?

Porque automatizar esta transformaci√≥n ayuda a tareas cr√≠ticas como:

- **Verificaci√≥n autom√°tica de programas** (probar que el c√≥digo cumple con lo especificado).
- **Pruebas formales** (testings guiados por propiedades).
- **Documentaci√≥n ejecutable** (donde los comentarios se vuelven parte verificable del programa).

Esta es una **tarea dura**, tradicionalmente realizada por expertos. Usar LLMs para esto es ambicioso, pero tambi√©n muy prometedor.

---

#### ¬øPor qu√© eligieron Dafny?

- Dafny es un lenguaje dise√±ado para **verificaci√≥n autom√°tica** de programas, con soporte para precondiciones, postcondiciones, invariantes, etc.
- Tiene un **verificador SMT incorporado**, que permite comprobar si las condiciones formales son l√≥gicamente correctas.
- Es usado frecuentemente en papers acad√©micos para tareas de autoformalizaci√≥n y synthesis.

Adem√°s, como el objetivo del paper es evaluar distribuciones sobre significados formales, usar Dafny permite chequear autom√°ticamente **equivalencia sem√°ntica** entre dos especificaciones (v√≠a SMT).

---
#### ¬øQu√© rol juega esta tarea en el paper?

Es el **caso de estudio** principal que se usa para aplicar el marco probabil√≠stico. Todo el an√°lisis (alineaci√≥n, concentraci√≥n, mejoras al TM) se ilustra a partir de esta tarea. Lo interesante es que:

- La tarea tiene un **ground truth claro** (las especificaciones correctas est√°n dadas).
- Se puede generar muchas salidas (por el LLM) y verificar si son **equivalentes** entre s√≠.
- Es posible categorizar errores comunes (por ejemplo, _postcondici√≥n d√©bil_, _omisi√≥n de propiedad clave_, _error de sintaxis_).

![[Pasted image 20250613102054.png]]

### C. Transference Models (TMs)

#### ¬øQu√© es un Transference Model?

Es una **abstracci√≥n de ingenier√≠a de software** que encapsula c√≥mo un sistema basado en LLM **transforma inputs en outputs** para una tarea particular.

- Es una **unidad funcional** dentro de un sistema m√°s grande.
- Se apoya en un LLM para llevar a cabo una transformaci√≥n (por ejemplo: docstring -> especificaci√≥n Dafny).
- Puede ser tan simple como un prompt, o m√°s complejo (con herramientas externas, reintentos, subm√≥dulos, etc)

``` python 
def doc2spec(docstring):
    prompt = "Given this docstring, write the pre/post conditions: " + docstring
    return LLM(prompt)
```

Este `doc2spec` ser√≠a un **TM**.

---
#### ¬øPor qu√© los TMs son estoc√°sticos?

Porque dependen de un LLM, y los LLMs son modelos **probabilisticos:** Dada una entrada $i$, el LLM puede producir distintas salidos $o$, cada una con una probabilidad. Formalmente:
$$T: \mathcal{I} \times \mathcal{O} \rightarrow \mathbb{R} \quad\text{donde}\quad T(i,o) = P(o|i)$$

O sea, el TM **define una distribuci√≥n** sobre salidas posibles para cada entrada. Esto depende fuertemente de:
- El LMM usado.
- El prompt.
- La **estrategia de decodificaci√≥n** (ej. sampling con temperatura, top-k, etc).

---

#### ¬øQu√© incluye un TM?

Un TM puede incluir no solo el llamado al LLM, sino tambi√©n:

- **Preprocesamiento** de la entrada (por ejemplo, limpiar el docstring).
- **Razonamiento externo** (RAG, vectores, heur√≠sticas).
- **Postprocesamiento** (validar la salida con el compilador).
- **Loops correctivos**, donde se itera sobre la salida si hubo errores.
- **Cadena de TMs** (uno genera una subparte, otro compone, etc.).

En el paper, muestran un **TM con m√∫ltiples etapas**:

1. Prompt inicial para generar la especificaci√≥n.
2. Si Dafny da error, se corrige usando el mensaje del compilador.
3. Adem√°s, usan un paso estilo RAG para buscar ejemplos similares (doc2vec + retrieval).

---

#### ¬øPor qu√© es tan importante este concepto?

Porque los autores **NO** est√°n interesados en estudiar al LLM directamente, sino en estudiar **los sistemas construidos con LLMs**. Es decir:

> "El LLM no es el sistema, sino una parte de un m√≥dulo m√°s grande que transforma datos para una tarea."

Entonces el **objeto de an√°lisis** del paper no es el modelo GPT o Gemini, sino el **TM** como componente de software. El marco propuesto eval√∫a c√≥mo el TM se comporta probabil√≠sticamente:
- Genera siempre la salida correcta?
- Qu√© tanta certeza tiene?
- Hay errores frecuentes con alta probabilidad?

---
#### Modelo funcional formal

Para reforzar: el TM se modela como una funci√≥n probabil√≠stica:

$$T(i, \cdot) = \text{una distribuci√≥n de probabilidad sobre salidas }o \text{ dado un input } i  $$
En otras palabras: **el TM act√∫a como una caja negra estoc√°stica** que recibe una entrada y escupe una distribuci√≥n sobre posibles resultados.

Lo que ellos proponen es: **muestrear esta caja negra** muchas veces con el mismo input, agrupar las salidas en clases de significado equivalentes y analizar esa distribuci√≥n.

---
### D. Distribution over Semantic Domains

Dado que los LLMs son funciones de estimaci√≥n de densidad sobre secuencias de tokens, los autores coinciden con otros trabajos que la probabilidad deber√≠a considerarse asignada a conceptos, no a cadenas de texto. Muchas veces hay m√∫ltiples (incluso infinitas) cadenas que representan una misma idea (por ejemplo, especificaciones l√≥gicamente equivalentes). En teor√≠a, esos corresponde a sumar las probabilidades asignadas a todas las cadenas dentro de cada clase de equivalencia. 

#### ¬øCu√°l es el problema de trabajar con cadenas de texto?

Los LLMs predicen secuencias de tokens (palabras o s√≠mbolos). Eso genera una **distribuci√≥n sobre strings como:**

```
"Returns the index of x" ‚Üí 20%  
"Find the position of x" ‚Üí 18%  
"Locate x in the array" ‚Üí 12%  
```

**Pero** estas cadenas pueden tener el **mismo significado**, aunque est√©n escritas distinto, entonces:

> Analizar solo la cadena m√°s frecuente o el token m√°s probable es **insuficiente** para entender qu√© est√° "pensando" el modelo.

---

#### ¬øQu√© proponen entonces?

Agrupar las salidas en **clases sem√°nticas equivalentes** (lo que llaman *meaning classes*). Cada clase agrupa todas las salidas que, aunque tengan forma distinta, expresan la **misma idea.** Por ejemplo si dos f√≥rmulas Dafny dicen: 

``` dafny
ensures exists i :: a[i] == x && forall j < i :: a[j] != x
```

``` dafny
ensures the returned index is the first occurrence of x
```

Estas pueden considerarse **sem√°nticamente equivalentes** (si ambas implican lo mismo). Por tanto, deber√≠an pertenecer a la misma clase. Entonces la **probabilidad total del "concepto"** deber√≠a ser:

$$P(\text{clase}) = \sum_{\text{strings en la clase}} P(\text{string})$$
---
#### ¬øPor qu√© este punto es crucial?

Porque al trabajar con LLMs:
- El modelo puede generar miles de variantes textuales **de una misma respuesta sem√°ntica**
- Si evalu√°s solo por exact matching o string similarityu, **subestim√°s la probabilidad del concepto correcto.**

üëâ Entonces, los autores abogan por construir distribuciones **no sobre cadenas**, sino sobre **dominios sem√°nticos abstractos**.

Este cambio de perspectiva es **clave** para su marco probabil√≠stico. Les permite:

- Medir correctamente alineaci√≥n con la verdad.
- Detectar errores sem√°nticos aunque el texto ‚Äúsuene bien‚Äù.
- Evaluar si el modelo es ‚Äúconfuso‚Äù o ‚Äúseguro‚Äù sem√°nticamente, no solo superficialmente.

---
### E. Computing Empirical Distributions of TMs

Dado que los TMs se comportan como procesos estoc√°sticos y no siempre se tiene acceso directo a las probabilidades del *next-token*, se propone aproximar la distribuci√≥n de salidas del TM usando una distribuci√≥n categ√≥rica emp√≠rica sobre las *meaning classes*. Estas clases se obtiene **re-ejecutando** el TM m√∫ltiples veces con el mismo input y agrupando las salidas segun una relaci√≥n de equivalencai sem√°ntica (por ejemplo, usando un SMT solver).

---

#### ¬øQu√© problemas enfrentan?

Idealmente, para cada input $i$, querr√≠amos conocer la distribucion completa $P(o | i)$, es decir, cu√°nta probabilidad asigna el TM a cada posible output. Pero eso **no es accesible directamente** por varias razones:

- La mayor√≠a de las APIs de LLM **no exponen la distribuci√≥n completa de tokens.**
- Incluso si lo hicieran, reconstruir la distribuci√≥n sobre *meaning classes*, supongo yo es **incomputable**.
- El espacio de posibles salidas $o \in \mathcal{O}$ es **enorme o infinito**, especialmente si se cuenta texto natural. 

---

#### ¬øQu√© soluci√≥n proponen?

Usar una **distribuci√≥n emp√≠rica** construida por muestreo:

1. Se re-ejecuta el TM varias veces con la **misma entrada** $i$. ‚ö†Ô∏è Cada ejecuci√≥n genera una salida distinta por el car√°cter estoc√°stico del LLM (sampling, temperatura, etc.).
2. Se recolectan todas las salidas $o_1, o_2, \dots, o_n$.
3. Se agrupan esas salidaas en **clases sem√°nticamente equivalentes**‚Äî _meaning classes_ 
	1. En autoformalizaci√≥n, esto se hace verificando **equivalencia l√≥gica** de las pre/post condiciones generadas.
	2. Si dos salidas son equivalentes seg√∫n un STM solver, pertenencen a la msima clase.
4. Se cuenta cu√°ntas veces apareci√≥ cada clase. Eso forma una **distribuci√≥n categ√≥rica emp√≠rica:**

$$\hat{P}_i(c) = \frac{\text{\# de veces que se gener√≥ la clase } c}{\text{total de ejecuciones}}$$
Esta es la distribuci√≥n sobre **conceptos**, no sobre cadenas de texto.

---
#### ¬øC√≥mo se agrupan las salidas?

Esto depende fuertemente de la tarea. En autoformalizaci√≥n, dos especificaciones son equivalentes si:

- Tienen la misma **sem√°ntica l√≥gica** (no importa si est√°n escritas distinto).
- Esto se eval√∫a usando **SMT solvers** (como Z3 o el backend de Dafny).

üìå Importante: si la salida tiene **error sint√°ctico**, se considera su propia clase (no se agrupa con ninguna otra).

---

#### ¬øQu√© significa "comportamiento estoc√°stico del TM"?

El TM puede comportarse como una caja negra:  
Cada vez que le das un input iii, responde con una salida diferente ooo, dependiendo de:

- El modelo base (GPT, Gemini, etc.).
- El prompt.
- Los hiperpar√°metros del decoding (temperatura, top-k, etc.).

Entonces, lo modelan como un **proceso aleatorio**:  
Repet√≠s muchas veces y ves qu√© patrones emergen.

Esto tambi√©n permite estudiar **certeza del modelo**: si siempre responde igual, hay concentraci√≥n; si reparte entre muchas clases, hay dispersi√≥n.

---

#### ¬øPor qu√© es importante este paso? 

Porque **toda la evaluaci√≥n de alineaci√≥n y concentraci√≥n** se basa en esta distribuci√≥n emp√≠rica:

- Si la clase correcta tiene la mayor frecuencia ‚áí **distribuci√≥n alineada.**
- Si una clase incorrecta domina ‚áí **concentraci√≥n pero desalineada (peligroso).**
- Si la distribuci√≥n est√° dispersa ‚áí **incertidumbre o ambig√ºedad**

---
#### Mis Preguntitas:

- ¬øCu√°ntas muestras son suficientes para estimar bien la distribuci√≥n? (En el estudio usan 30).
- ¬øQu√© pasa si el solver SMT falla o es muy lento? ¬øC√≥mo escalar esto?
- ¬øC√≥mo definir equivalencia sem√°ntica en tareas donde no hay l√≥gica formal?
- ¬øQu√© impacto tienen los hiperpar√°metros del LLM (p. ej. temperatura = 0.7 vs. 1.0) sobre la distribuci√≥n?

---

## III.  The Framework 
### A. Assumptions, Hypothesis and Rationale

Los autores presentan **su hip√≥tesis central**, los supuestos que hacen sobre los TMs y los fundamentos que justifican su enfoque probabil√≠stico. Quieren responder esta pregunta:

> ¬øPor qu√© analizar la distribuci√≥n de significados generada por un TM es una buena forma de evaluar y mejorar sistemas con LLMs?.

#### Supuesto 1:

> La mayoria de los sistemas LLM pueden modelarse con uno o m√°s TMs

Significa que cualquier sistema que use un LLM (incluso los complejos) puede representarse como un conjunto de **transformaciones de entrada a salida** que dependen del modelo.

- Un chatbot ‚Üí TM: prompt ‚Üí respuesta.
- Un sistema de generaci√≥n de c√≥digo ‚Üí TM: docstring ‚Üí c√≥digo.
- Un sistema de razonamiento encadenado ‚Üí varios TMs combinados.

#### Supuesto 2:

> Las tareas de los TMs pueden entenderse como tareas humanas espec√≠ficas.

Esto quiere decir que cada TM **representra una tarea clara y humana-interpretable** (como generar especificaciones, traducir c√≥digo, etc.), y sus salidas pueden evaluarse en t√©rminos de *clases de resultados equivalentes.* Esto es clave porque:

- Permite definir si **una salida es correcta o no.**
- Permite agrupar salidas **sem√°nticamente equivalentes** (*meaning classes*)
- Permite evaluar el modelo de forma significativa, no solo superficial.

#### Hip√≥tesis Principal

> El comportamiento de un TM se puede caracterizar como una **distribuci√≥n probabil√≠stica** sobre conceptos.

Es decir, *dado un input $i$, el TM genera una distribuci√≥n sobre distintas **clases de significados posibles** (no solo strings)*

Y lo que importa no es solo si genera alguna respuesta correcta, sino **cu√°nta probabilidad se le asigna.** Esta visi√≥n lleva a **dos nociones clave:**
- **Alineaci√≥n:** ¬øla clase m√°s probable es la correcta?
- **Concentraci√≥n:** ¬øla distribuci√≥n est√° concentrada en una clase o es difusa?

#### Hip√≥tesis Complementaria

> Los casos **concentrados** y **desalineados** (o sea, equivocaci√≥n con mucha confianza) son los m√°s peligrosos.

Estos son los que:
- Pueden **enga√±ar mecanismos de detecci√≥n de alucinaci√≥n** basados en certeza (e.g. si algo tiene alta probabilidad, parece confiable).
- Producen **errores sistem√°ticos** dif√≠ciles de detectar.
- Son candidatos ideales para el an√°lisis y reingenier√≠a del TM.

#### Relaci√≥n con *refinamiento* en verificaci√≥n formal

Proponen que mejorar un TM se parece a **refinar un programa** en el sentido de subtipado de comportamiento (como Liskov/Wing). Un TM mejorado, deber√≠a:

- Generar m√°s outputs correctos con alta certeza.
- Reducir los casos donde se confunde con alta certeza.
- No romper los casos donde ya funcionaba bien.

Esto sugiere que **una mejora no debe da√±ar lo que ya funciona,** sino ampliar y corregir su comportamiento, una visi√≥n **ingenieril** y **formal** del proceso de mejora. 

#### Hip√≥tesis adicional

> Cuando una clase incorrecta es la **dominante,** su **"tipo de error"** se puede **verbalizar en t√©rminos espec√≠ficos de la tarea**

Por ejemplo, en autoformalizaci√≥n, hay tipos de errores conocidos:
- Precondici√≥n muy d√©bil
- Postcondici√≥n incorrecta
- Sintaxis inv√°lida.

Cuando un TM se equivoca con alta certeza, muchas veces lo hace de una forma que **esta dentro de una de estas categor√≠as conocidas**. Esto permite **diagnosticar fallos autom√°ticamente** y **guiar ajustes espec√≠ficos**.




### B. Defining Improvement

#### ¬øQu√© es una distribuci√≥n alineada?

Para un input $i$, se generan m√∫ltiples outputs y se agrupan en **clases de significado.**

- Se dice que la distribuci√≥n es **alineada** si la clase de significado **con mayor probabilidad** (la clase ganadora), es **correcta.**
- Si la clase correcta est√° presente pero no es la m√°s probable ‚áí **desalineada**.
- Si la clase correcta **ni siquiera aparece** en la muestra ‚áí **fuertemente desalineada**.

#### ¬øQu√© es una distribuci√≥n concentrada?

Una distribuci√≥n se considera **concentrada** si la clase ganadora tiene **probabilidad mayor o igual que la suma del resto:**

$$P(\text{clase ganadora}) \geq \sum_{\text{c}\neq\text{ganadora}} P(\text{c})$$

Alternativamente, podr√≠amos decir que tiene m√°s del 50% de la masa total. Esto es una **aproximaci√≥n pragm√°tica** a la noci√≥n de "certeza sem√°ntica" del modelo. 

#### Clasificaci√≥n de outputs seg√∫n estas dos dimensiones

Esto genera una **matriz 2x2** de escenarios para un input:

|                 | **Concentrada** | **No Concentrada** |
| --------------- | --------------- | ------------------ |
| **Alineada**    | ‚úÖ Ideal         | ‚ö†Ô∏è Incierto        |
| **Desalineada** | ‚ùå Peligroso     | ‚ùì Ambiguo          |
- **Concentrada + alineada**: el TM sabe lo que hace.
- **Concentrada + desalineada**: error con alta confianza ‚Üí muy peligroso.
- **No concentrada**: el modelo est√° confundido o disperso ‚Üí menos riesgoso pero requiere atenci√≥n.

#### ¬øQu√© significa que un TM mejora a otro?

Supongamos que tenemos dos TMS, $t$ y 
## IV. Illustrative Results on Autoformalization

### Table I
### Table II

### Table III

## V. Related Work

## VI. Conclusions and Future Work

### A. The Case of Agentic AI
